{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a2c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gg\n"
     ]
    }
   ],
   "source": [
    "print(\"gg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d5a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.10.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets) (2025.10.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6915e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigalab/python-llm-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"  # GPT-2 Small\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9af55",
   "metadata": {},
   "source": [
    "## secret wiki datset for grammer ve encylopedia knowlede artırmak için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f79bf9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_ds is okay\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "wiki_ds_sample=15000\n",
    "def format_wiki_with_topic(example):\n",
    "    text = example[\"text\"].strip()\n",
    "    \n",
    "    # Wikitext başlıklarını bulmaya çalışalım (örn: = Robert Oppenheimer =)\n",
    "    # Eğer başlık bulamazsak ilk cümleyi 'topic' kabul edelim\n",
    "    title_search = re.search(r'=\\s(.*?)\\s=', text)\n",
    "    \n",
    "    if title_search:\n",
    "        topic = title_search.group(1)\n",
    "        # Başlık kısmını temizleyip sadece içeriği 'output' yapalım\n",
    "        content = text.replace(title_search.group(0), \"\").strip()\n",
    "    else:\n",
    "        # Eğer özel bir başlık yoksa ilk 5 kelimeyi konu yapalım\n",
    "        words = text.split()\n",
    "        topic = \" \".join(words[:5]) + \"...\"\n",
    "        content = text\n",
    "\n",
    "    # Eğer içerik çok kısaysa veriyi atla\n",
    "    if len(content) < 100:\n",
    "        return {\"instruction\": None, \"output\": None}\n",
    "\n",
    "    return {\n",
    "        \"instruction\": f\"about {topic}.\",\n",
    "        \"output\": content\n",
    "    }\n",
    "\n",
    "# Veriyi yükle ve yeni fonksiyonla işle\n",
    "wiki_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "wiki_ds = wiki_ds.map(format_wiki_with_topic)\n",
    "wiki_ds = wiki_ds.filter(lambda x: x[\"instruction\"] is not None)\n",
    "wiki_ds = wiki_ds.shuffle(seed=42).select(range(wiki_ds_sample))\n",
    "wiki_ds = wiki_ds.remove_columns([\"text\"])\n",
    "wiki_ds_split = wiki_ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_wiki_ds = wiki_ds_split[\"train\"]\n",
    "val_wiki_ds = wiki_ds_split[\"test\"]\n",
    "print(\"wiki_ds is okay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97809d72",
   "metadata": {},
   "source": [
    "## secret daily dialogue dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f742617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_daily is okay\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds_daily_sample=5000\n",
    "def parse_dialogues_file(file_path):\n",
    "    pairs = []\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Satırı __eou__ ifadesinden parçala ve boşlukları temizle\n",
    "            turns = [t.strip() for t in line.split(\"__eou__\") if t.strip()]\n",
    "            \n",
    "            # Konuşma akışındaki her bir ikiliyi (turn) çıkarıyoruz\n",
    "            # Örn: [T1, T2, T3] -> (T1, T2) ve (T2, T3) çiftleri oluşur\n",
    "            for i in range(len(turns) - 1):\n",
    "                pairs.append({\n",
    "                    \"instruction\": turns[i],\n",
    "                    \"output\": turns[i+1]\n",
    "                })\n",
    "                \n",
    "    return pairs\n",
    "\n",
    "# 1. Dosyayı parse et\n",
    "dialogue_pairs = parse_dialogues_file(\"dialogues_train.txt\")\n",
    "\n",
    "# 2. Listeyi Hugging Face Dataset formatına çevir\n",
    "ds_daily_custom = Dataset.from_list(dialogue_pairs)\n",
    "\n",
    "# 3. Senin sütun yapına uygun hale getir (Zaten yukarıda öyle yaptık)\n",
    "# Sütun isimlerini garantiye alalım\n",
    "ds_daily_custom = ds_daily_custom.select_columns([\"instruction\", \"output\"])\n",
    "ds_daily_custom = ds_daily_custom.shuffle(seed=42)\n",
    "ds_daily_final = ds_daily_custom.select(range(ds_daily_sample))\n",
    "ds_daily_split = ds_daily_final.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds_daily = ds_daily_split[\"train\"]\n",
    "val_ds_daily = ds_daily_split[\"test\"]\n",
    "\n",
    "\n",
    "print(\"ds_daily is okay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77770605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_chat is okay\n",
      "ds_code is okay\n",
      "ds_math is okay\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# ds_chat_sample=15000\n",
    "# ds_code_sample=18000\n",
    "# ds_math_sample=5000\n",
    "# ds_daily_sample=5000\n",
    "# # --- 1. SOHBET VERİSİ (Örn: UltraChat) ---\n",
    "# # Orijinal sütunları: 'prompt' ve 'answers' diyelim.\n",
    "# ds_chat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{ds_chat_sample}]\")\n",
    "# ds_chat = ds_chat.map(lambda x: {\n",
    "#     \"instruction\": x[\"prompt\"], \n",
    "#     \"output\": x[\"messages\"][1][\"content\"] # Veri yapısına göre seçiyoruz\n",
    "# }, remove_columns=ds_chat.column_names)\n",
    "# print(\"ds_chat is okay\")\n",
    "\n",
    "# # --- 2. KOD VERİSİ (Örn: Alpaca Code) ---\n",
    "# # Orijinal sütunları: 'instruction', 'input', 'output'\n",
    "# ds_code = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=f\"train[:{ds_code_sample}]\")\n",
    "# ds_code = ds_code.map(lambda x: {\n",
    "#     \"instruction\": x[\"instruction\"] + (\"\\n\" + x[\"input\"] if x[\"input\"] else \"\"),\n",
    "#     \"output\": x[\"output\"]\n",
    "# }, remove_columns=ds_code.column_names)\n",
    "# print(\"ds_code is okay\")\n",
    "# # --- 3. MATEMATİK VERİSİ (Örn: GSM8K) ---\n",
    "# # Orijinal sütunları: 'question', 'answer'\n",
    "# ds_math = load_dataset(\"gsm8k\", \"main\", split=f\"train[:{ds_math_sample}]\")\n",
    "# ds_math = ds_math.map(lambda x: {\n",
    "#     \"instruction\": x[\"question\"],\n",
    "#     \"output\": x[\"answer\"]\n",
    "# }, remove_columns=ds_math.column_names)\n",
    "# print(\"ds_math is okay\")\n",
    "\n",
    "# ds_daily = load_dataset(\"daily_dialog\", split=f\"train[:{ds_daily_sample}]\")\n",
    "\n",
    "# # DailyDialog'da mesajlar liste halindedir. \n",
    "# # En basit yöntem: İlk mesajı 'instruction', ikinciyi 'output' yapmak.\n",
    "# def format_daily(example):\n",
    "#     # Eğer diyalog en az 2 mesaj içeriyorsa formatla\n",
    "#     if len(example[\"dialog\"]) >= 2:\n",
    "#         return {\n",
    "#             \"instruction\": example[\"dialog\"][0],\n",
    "#             \"output\": example[\"dialog\"][1]\n",
    "#         }\n",
    "#     else:\n",
    "#         # Boş veya tek mesajlıları temizlemek için işaretle\n",
    "#         return {\"instruction\": None, \"output\": None}\n",
    "\n",
    "# ds_daily = ds_daily.map(format_daily)\n",
    "# # None olanları (tek mesajlı diyalogları) temizle ve sütunları eşitle\n",
    "# ds_daily = ds_daily.filter(lambda x: x[\"instruction\"] is not None)\n",
    "# ds_daily = ds_daily.remove_columns([col for col in ds_daily.column_names if col not in [\"instruction\", \"output\"]])\n",
    "\n",
    "# print(\"ds_daily is okay\")\n",
    "# # --- ŞİMDİ BİRLEŞTİREBİLİRİZ ---\n",
    "# # Artık hepsinin sadece 2 sütunu var: 'instruction' ve 'output'\n",
    "# dataset = concatenate_datasets([ds_chat, ds_code, ds_math,wiki_ds,ds_daily,])\n",
    "# # Veri setlerini tek bir havuzda birleştirme\n",
    "# dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cbaec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_chat is okay\n",
      "ds_code is okay\n",
      "ds_math is okay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 15011/15011 [00:00<00:00, 331366.86 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 25289.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolly hazır! 10000 örnek eklendi.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "ds_chat_sample=11000\n",
    "ds_code_sample=10000\n",
    "ds_math_sample=4000\n",
    "ds_dolly_sample=10000\n",
    "\n",
    "# --- 1. SOHBET VERİSİ (Örn: UltraChat) ---\n",
    "# Orijinal sütunları: 'prompt' ve 'answers' diyelim.\n",
    "ds_chat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{ds_chat_sample}]\")\n",
    "ds_chat = ds_chat.map(lambda x: {\n",
    "    \"instruction\": x[\"prompt\"], \n",
    "    \"output\": x[\"messages\"][1][\"content\"] # Veri yapısına göre seçiyoruz\n",
    "}, remove_columns=ds_chat.column_names)\n",
    "ds_chat_split = ds_chat.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds_chat = ds_chat_split[\"train\"]\n",
    "val_ds_chat = ds_chat_split[\"test\"]\n",
    "print(\"ds_chat is okay\")\n",
    "\n",
    "# --- 2. KOD VERİSİ (Örn: Alpaca Code) ---\n",
    "# Orijinal sütunları: 'instruction', 'input', 'output'\n",
    "ds_code = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=f\"train[:{ds_code_sample}]\")\n",
    "ds_code = ds_code.map(lambda x: {\n",
    "    \"instruction\": x[\"instruction\"] + (\"\\n\" + x[\"input\"] if x[\"input\"] else \"\"),\n",
    "    \"output\": x[\"output\"]\n",
    "}, remove_columns=ds_code.column_names)\n",
    "ds_code_split = ds_code.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds_code = ds_code_split[\"train\"]\n",
    "val_ds_code = ds_code_split[\"test\"]\n",
    "print(\"ds_code is okay\")\n",
    "\n",
    "# --- 3. MATEMATİK VERİSİ (Örn: GSM8K) ---\n",
    "# Orijinal sütunları: 'question', 'answer'\n",
    "ds_math = load_dataset(\"gsm8k\", \"main\", split=f\"train[:{ds_math_sample}]\")\n",
    "ds_math = ds_math.map(lambda x: {\n",
    "    \"instruction\": x[\"question\"],\n",
    "    \"output\": x[\"answer\"]\n",
    "}, remove_columns=ds_math.column_names)\n",
    "ds_math_split = ds_math.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds_math = ds_math_split[\"train\"]\n",
    "val_ds_math = ds_math_split[\"test\"]\n",
    "print(\"ds_math is okay\")\n",
    "\n",
    "\n",
    "# --- 6. DOLLY EKLEME ---\n",
    "ds_dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=f\"train[:{ds_dolly_sample}]\")\n",
    "\n",
    "def format_dolly(example):\n",
    "    # 'context' varsa onu da instruction'a ekleyelim ki model bilgi kullanmayı öğrensin\n",
    "    instruction = example[\"instruction\"]\n",
    "    if example.get(\"context\"):\n",
    "        instruction += \"\\nContext: \" + example[\"context\"]\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": example[\"response\"]\n",
    "    }\n",
    "\n",
    "ds_dolly = ds_dolly.map(format_dolly, remove_columns=ds_dolly.column_names)\n",
    "ds_dolly_split = ds_dolly.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds_dolly = ds_dolly_split[\"train\"]\n",
    "val_ds_dolly = ds_dolly_split[\"test\"]\n",
    "print(f\"Dolly hazır! {len(ds_dolly)} örnek eklendi.\")\n",
    "\n",
    "\n",
    "# --- ŞİMDİ BİRLEŞTİREBİLİRİZ ---\n",
    "# Artık hepsinin sadece 2 sütunu var: 'instruction' ve 'output'\n",
    "dataset = concatenate_datasets([train_ds_chat, train_ds_code, train_ds_math, train_wiki_ds, train_ds_daily,train_ds_dolly])\n",
    "\n",
    "# Veri setlerini tek bir havuzda birleştirme\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "val_dataset_dict = {\n",
    "    \"chat\": val_ds_chat,\n",
    "    \"code\": val_ds_code,\n",
    "    \"math\": val_ds_math,\n",
    "    \"daily\": val_ds_daily,\n",
    "    \"wiki\": val_wiki_ds,\n",
    "    \"dolly\": val_ds_dolly\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78de7fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52250/52250 [00:04<00:00, 11930.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"gpt2\" # veya \"gpt2-medium\", \"gpt2-large\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # GPT-2'de pad_token tanımlı değildir, eos_token'ı pad_token olarak atıyoruz\n",
    "# tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# Veri setini GPT-2'nin anlayacağı formata sokma\n",
    "def format_for_gpt2(example):\n",
    "    # Soru ve cevabı özel belirteçlerle birleştiriyoruz\n",
    "    # GPT-2 için genellikle <|endoftext|> kullanılır ama araya ayıraç koymak iyidir\n",
    "    full_text = f\"Question: {example['instruction']}\\nAnswer: {example['output']}{tokenizer.eos_token}\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "# Bir önceki adımda oluşturduğun combined_dataset'i kullanıyoruz\n",
    "gpt2_dataset = dataset.map(format_for_gpt2, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:00<00:00, 13881.44 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 5082.48 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2045' max='2045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2045/2045 52:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.042800</td>\n",
       "      <td>1.780606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.044400</td>\n",
       "      <td>1.781915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.046000</td>\n",
       "      <td>1.782603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.044400</td>\n",
       "      <td>1.782934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.042900</td>\n",
       "      <td>1.783075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2045, training_loss=2.0441091374255045, metrics={'train_runtime': 3131.7632, 'train_samples_per_second': 83.419, 'train_steps_per_second': 0.653, 'total_flos': 6.826254336e+16, 'train_loss': 2.0441091374255045, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "mpath=\"./gpt2-multitask-model-new/checkpoint-818\"\n",
    "mpath=\"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(mpath)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mpath)\n",
    "# Veriyi tokenlara ayırma (Tokenization)\n",
    "def tokenize_function(examples):\n",
    "    tok = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tok[\"labels\"] = tok[\"input_ids\"].copy()  # labels ekleniyor\n",
    "    return tok\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "#gpt2_dataset = gpt2_dataset.shuffle()\n",
    "tokenized_dataset = gpt2_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Tüm validasyon setlerini önce format et, sonra tokenize et\n",
    "tokenized_vals = {\n",
    "    name: ds.map(format_for_gpt2, remove_columns=ds.column_names).map(tokenize_function, batched=True, remove_columns=[\"text\"]) \n",
    "    for name, ds in val_dataset_dict.items()\n",
    "}\n",
    "\n",
    "# Modeli yükleme\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Eğitim ayarları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-multitask-model-new\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,            # Veri üzerinden 3 tur geç\n",
    "    per_device_train_batch_size=32, # GPU belleğine göre artırabilirsin\n",
    "    save_steps=300,\n",
    "    logging_steps=100,\n",
    "    eval_steps=300,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=3e-6,            # GPT-2 için ideal öğrenme oranı\n",
    "    weight_decay=0.1,\n",
    "    no_cuda=False,\n",
    "    report_to=\"none\",\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Data Collator: Verileri modelin okuyabileceği paketler haline getirir\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_vals[\"chat\"],  # İstersen tek bir validasyon seti ile başlayabilirsin\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Eğitimi başlat\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0776eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Write a python function to sort a list.\n",
      "Answer: def sort_list(lst):\n",
      "    for i in range(len(lst)):\n",
      "        min_idx = i\n",
      "        for j in range(\n",
      "Question: What is 2 + 2?\n",
      "Answer: 2 + 2 is a number that is divisible by 3. Therefore, it is divisible by 3 / 3 = <<2/3=1>>1.\n",
      "#### 1 is the number that is divisible by 3. Therefore, it is\n",
      "Question: Who is the Obama?\n",
      "Answer: The Obama is the first president elected by the American people. He is the first president elected by the American people and is the first to be elected by the popular vote. The president is elected by the people and is accountable to the people for their decisions\n",
      "Question: About world war 2?\n",
      "Answer: World war 2 was a conflict between the Soviet Union and Nazi Germany that lasted from 1939 to 1945. The conflict was fought between the Soviet Union and Nazi Germany over control of the Black Sea Fleet and the control of the Mediterranean Sea. The Soviet Union was\n",
      "Question: write a short poem about AI.\n",
      "Answer: In the world of technology,\n",
      "We can dream, we can dream.\n",
      "\n",
      "We can dream, we can dream.\n",
      "\n",
      "We can dream, we can dream.\n",
      "\n",
      "We can dream, we can dream.\n",
      "\n",
      "We can dream\n"
     ]
    }
   ],
   "source": [
    "def ask_gpt2(question):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # GPU varsa\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Test\n",
    "ask_gpt2(\"Write a python function to sort a list.\")\n",
    "ask_gpt2(\"What is 2 + 2?\")\n",
    "ask_gpt2(\"Who is the Obama?\")\n",
    "ask_gpt2(\"About world war 2?\")\n",
    "ask_gpt2(\"write a short poem about AI.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
